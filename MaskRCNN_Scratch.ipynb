{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskRCNN_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tJ93Dd0_DG2P"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YVODNaqprrL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "import torchvision\n",
        "from torchsummary import summary\n",
        "import torch.utils as utils\n",
        "from torchvision.ops.roi_align import RoIAlign\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from xml.dom import minidom\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from sklearn.feature_extraction.image import extract_patches_2d\n",
        "from skimage.draw import polygon\n",
        "from scipy.io import savemat\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F_8e_g8bzG-"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1jiZIr64TaavSvLKiq1a-wGksskB18zR0' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1jiZIr64TaavSvLKiq1a-wGksskB18zR0\" -O monuseg.zip && rm -rf /tmp/cookies.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao1To9fpCVxA"
      },
      "source": [
        "!unzip \"/content/monuseg.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBayc6I2CXpD"
      },
      "source": [
        "%%shell\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../\n",
        "\n",
        "pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBsYDjxPChi0"
      },
      "source": [
        "def generate_masks(shape, xml_file):\n",
        "\t\"\"\"\n",
        "\tGiven the image shape and path to annotations (xml file),\n",
        "\tgenerate a bit mask with the region inside a contour being white\n",
        "\tshape: The image shape on which bit mask will be made\n",
        "\txml_file: path relative to the current working directory\n",
        "\twhere the xml file is present\n",
        "\tReturns: A image of given shape with region inside contour being white..\n",
        "\t\"\"\"\n",
        "\t# DOM object created by the minidom parser\n",
        "\txDoc = minidom.parse(xml_file)\n",
        "\n",
        "\t# List of all Region tags\n",
        "\tregions = xDoc.getElementsByTagName('Region')\n",
        "\n",
        "\t# List which will store the vertices for each region\n",
        "\txy = []\n",
        "\tfor region in regions:\n",
        "\t\tvertices = region.getElementsByTagName('Vertex')\n",
        "\n",
        "\t\t# The vertices of a region will be stored in a array\n",
        "\t\tvw = np.zeros((len(vertices), 2))\n",
        "\n",
        "\t\tfor index, vertex in enumerate(vertices):\n",
        "\t\t\t# Storing the values of x and y coordinate after conversion\n",
        "\t\t\tvw[index][0] = float(vertex.getAttribute('X'))\n",
        "\t\t\tvw[index][1] = float(vertex.getAttribute('Y'))\n",
        "\n",
        "\t\t# Append the vertices of a region\n",
        "\t\txy.append(np.int32(vw))\n",
        "\n",
        "\t# Creating a completely black image\n",
        "\tmask = np.zeros(shape, np.int32)\n",
        "\n",
        "\t# For each contour, fills the area inside it\n",
        "\t# Warning: If a list of contours is passed, overlapping regions get buggy output\n",
        "\t# Comment out the below line to check, and if the bug is fixed use this\n",
        "\t# cv2.drawContours(mask, xy, -1, (255,255,255), cv2.FILLED)\n",
        "\tj=1\n",
        "\tfor i,contour in enumerate(xy):\n",
        "\t\tif(xml_file.find('-')>=0):\n",
        "\t\t\tr,c = polygon(np.array(xy[i])[:,1],np.array(xy[i])[:,0],shape=shape)\n",
        "\t\telse:\n",
        "\t\t\tr,c = np.array(xy[i])[:,1],np.array(xy[i])[:,0]\n",
        "\n",
        "\t\tmask[r,c]=i+1\t\n",
        "\treturn mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl52bMm1CoVt"
      },
      "source": [
        "def make_patches_img(img_inp,filename,folder):\n",
        "    trans = transforms.ToPILImage()\n",
        "    trans_tensor = transforms.ToTensor()\n",
        "    img = trans_tensor(img_inp)\n",
        "    kernel = 250\n",
        "    ## num = 16\n",
        "    patch1 = trans(img[:,0:kernel, 0:kernel])\n",
        "    patch2 = trans(img[:,0:kernel, kernel:500])\n",
        "    patch3 = trans(img[:,0:kernel, (kernel+250):750])\n",
        "    patch4 = trans(img[:,0:kernel, (kernel+500):1000])\n",
        "    patch5 = trans(img[:,kernel:500, 0:kernel])\n",
        "    patch6 = trans(img[:,(kernel+250):750, 0:kernel])\n",
        "    patch7 =  trans(img[:,(kernel+500):1000, 0:kernel])\n",
        "    patch8 = trans(img[:,kernel:500, kernel:500])\n",
        "    patch9 = trans(img[:,kernel:500, (kernel+250):750])\n",
        "    patch10 = trans(img[:,kernel:500, (kernel+500):1000])\n",
        "    patch11 = trans(img[:,(kernel+250):750, kernel:500])\n",
        "    patch12 = trans(img[:,(kernel+500):1000, kernel:500])\n",
        "    patch13 = trans(img[:,(kernel+250):750, (kernel+250):750])\n",
        "    patch14 = trans(img[:,(kernel+250):750, (kernel+500):1000])\n",
        "    patch15 = trans(img[:,(kernel+500):1000, (kernel+250):750])\n",
        "    patch16 = trans(img[:,(kernel+500):1000,(kernel+500):1000])\n",
        "\n",
        "    #save the images in seperate files in a new folder\n",
        "    path1 = os.path.join(folder,'Tissue Images',\"1_\"+filename)\n",
        "    patch1.save(path1)\n",
        "    path2 = os.path.join(folder,'Tissue Images',\"2_\"+filename)\n",
        "    patch2.save(path2)\n",
        "    path3 = os.path.join(folder,'Tissue Images',\"3_\"+filename)\n",
        "    patch3.save(path3)\n",
        "    path4 = os.path.join(folder,'Tissue Images',\"4_\"+filename)\n",
        "    patch4.save(path4)\n",
        "    path5 = os.path.join(folder,'Tissue Images',\"5_\"+filename)\n",
        "    patch5.save(path5)\n",
        "    path6 = os.path.join(folder,'Tissue Images',\"6_\"+filename)\n",
        "    patch6.save(path6)\n",
        "    path7 = os.path.join(folder,'Tissue Images',\"7_\"+filename)\n",
        "    patch7.save(path7)\n",
        "    path8 = os.path.join(folder,'Tissue Images',\"8_\"+filename)\n",
        "    patch8.save(path8)\n",
        "    path9 = os.path.join(folder,'Tissue Images',\"9_\"+filename)\n",
        "    patch9.save(path9)\n",
        "    path10 = os.path.join(folder,'Tissue Images',\"10_\"+filename)\n",
        "    patch10.save(path10)\n",
        "    path11 = os.path.join(folder,'Tissue Images',\"11_\"+filename)\n",
        "    patch11.save(path11)\n",
        "    path12 = os.path.join(folder,'Tissue Images',\"12_\"+filename)\n",
        "    patch12.save(path12)\n",
        "    path13 = os.path.join(folder,'Tissue Images',\"13_\"+filename)\n",
        "    patch13.save(path13)\n",
        "    path14 = os.path.join(folder,'Tissue Images',\"14_\"+filename)\n",
        "    patch14.save(path14)\n",
        "    path15 = os.path.join(folder,'Tissue Images',\"15_\"+filename)\n",
        "    patch15.save(path15)\n",
        "    path16 = os.path.join(folder,'Tissue Images',\"16_\"+filename)\n",
        "    patch16.save(path16)\n",
        "\n",
        "    return \n",
        " \n",
        "def make_patches_mask(mask,filename, folder):\n",
        "\n",
        "    kernel = 250\n",
        "    ## num = 16\n",
        "    patch1 = mask[0:kernel,0:kernel]\n",
        "    patch2 = mask[0:kernel,kernel:500]\n",
        "    patch3 = mask[0:kernel,(kernel+250):750]\n",
        "    patch4 = mask[0:kernel, (kernel+500):1000]\n",
        "    patch5 = mask[kernel:500, 0:kernel]\n",
        "    patch6 = mask[(kernel+250):750, 0:kernel]\n",
        "    patch7 = mask[(kernel+500):1000, 0:kernel]\n",
        "    patch8 = mask[kernel:500, kernel:500]\n",
        "    patch9 = mask[kernel:500, (kernel+250):750]\n",
        "    patch10 = mask[kernel:500, (kernel+500):1000]\n",
        "    patch11 = mask[(kernel+250):750, kernel:500]\n",
        "    patch12 = mask[(kernel+500):1000, kernel:500]\n",
        "    patch13 = mask[(kernel+250):750, (kernel+250):750]\n",
        "    patch14 = mask[(kernel+250):750, (kernel+500):1000]\n",
        "    patch15 = mask[(kernel+500):1000, (kernel+250):750]\n",
        "    patch16 = mask[(kernel+500):1000,(kernel+500):1000]\n",
        "\n",
        "    #save the masks in seperate files in a new folder\n",
        "    path1 = os.path.join(folder,'Annotations',\"1_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path1, patch1, delimiter=',')\n",
        "    path2 = os.path.join(folder,'Annotations',\"2_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path2, patch2, delimiter=',')\n",
        "    path3 = os.path.join(folder,'Annotations',\"3_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path3, patch3, delimiter=',')\n",
        "    path4 = os.path.join(folder,'Annotations',\"4_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path4, patch4, delimiter=',')\n",
        "    path5 = os.path.join(folder,'Annotations',\"5_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path5, patch5, delimiter=',')\n",
        "    path6 = os.path.join(folder,'Annotations',\"6_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path6, patch6, delimiter=',')\n",
        "    path7 = os.path.join(folder,'Annotations',\"7_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path7, patch7, delimiter=',')\n",
        "    path8 = os.path.join(folder,'Annotations',\"8_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path8, patch8, delimiter=',')\n",
        "    path9 = os.path.join(folder,'Annotations',\"9_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path9, patch9, delimiter=',')\n",
        "    path10 = os.path.join(folder,'Annotations',\"10_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path10, patch10, delimiter=',')\n",
        "    path11 = os.path.join(folder,'Annotations',\"11_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path11, patch11, delimiter=',')\n",
        "    path12 = os.path.join(folder,'Annotations',\"12_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path12, patch12, delimiter=',')\n",
        "    path13 = os.path.join(folder,'Annotations',\"13_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path13, patch13, delimiter=',')\n",
        "    path14 = os.path.join(folder,'Annotations',\"14_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path14, patch14, delimiter=',')\n",
        "    path15 = os.path.join(folder,'Annotations',\"15_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path15, patch15, delimiter=',')\n",
        "    path16 = os.path.join(folder,'Annotations',\"16_\"+filename.split('.')[0]+'.csv')\n",
        "    np.savetxt(path16, patch16, delimiter=',')\n",
        "    \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwGODAWjCtB8"
      },
      "source": [
        "folder = '/content/MonuSeg'\n",
        "!rm -r MonuSeg\n",
        "\n",
        "!mkdir 'MonuSeg'\n",
        "!mkdir 'MonuSeg/Tissue Images'\n",
        "!mkdir 'MonuSeg/Annotations'\n",
        "\n",
        "img_directory = '/content/MoNuSegTrainingData/Tissue Images'\n",
        "mask_directory = '/content/MoNuSegTrainingData/Annotations'\n",
        "\n",
        "for imgfilename in os.listdir(img_directory):\n",
        "    img_file = os.path.join(img_directory, imgfilename)\n",
        "    if(img_file.endswith('.tif')):\n",
        "      img = Image.open(img_file).convert(\"RGB\")\n",
        "      nrow, ncol = img.size\n",
        "      shape = (nrow, ncol)\n",
        "      make_patches_img(img, imgfilename,folder)\n",
        "    \n",
        "\n",
        "for maskfilename in os.listdir(mask_directory):\n",
        "    mask_file = os.path.join(mask_directory, maskfilename)\n",
        "    if(mask_file.endswith('.xml')):\n",
        "      xml_file = mask_file\n",
        "      mask = generate_masks(shape, xml_file)\n",
        "      make_patches_mask(mask, maskfilename,folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGFoBDfqIzAJ"
      },
      "source": [
        "class Monuseg_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"Tissue Images\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"Annotations\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"Tissue Images\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"Annotations\", self.masks[idx])\n",
        "        #print(mask_path)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img.show()\n",
        "        #------\n",
        "        mask = np.loadtxt(mask_path, delimiter=',')\n",
        "\n",
        "        print(img_path)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymax = np.max(pos[0])\n",
        "            if(xmin == xmax):\n",
        "              xmin = xmin-1\n",
        "              xmax = xmax+1\n",
        "            if(ymin == ymax):\n",
        "              ymin = ymin-1\n",
        "              ymax = ymax+1\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n",
        "        \n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64) # the label for each bounding box\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8) # The segmentation masks for each one of the objects\n",
        "\n",
        "        image_id = torch.tensor([idx]) #  an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # The area of the bounding box\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, boxes,labels,masks,area\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR9Ci3YZDkM5"
      },
      "source": [
        "import transforms as T\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "\n",
        "    return T.Compose(transforms)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5skUM2FDr8O"
      },
      "source": [
        "dataset = Monuseg_Dataset('/content/MonuSeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hJjXT_Pp5yD"
      },
      "source": [
        "### Data Processing \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lzLPgkz6UZ4"
      },
      "source": [
        "torch.seed()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "os.chdir('/content/drive/My Drive/MoNuSegTrainingData/')\n",
        "\n",
        "ROOT_DIR = os.path.abspath(\"../../\")\n",
        "sys.path.append(ROOT_DIR)\n",
        "\n",
        "# Data Path\n",
        "TRAIN_PATH_X = 'MoNuSegTrainingData/Data/'\n",
        "TRAIN_PATH_Y_Binary = 'MoNuSegTrainingData/Binary_masks/'\n",
        "TRAIN_PATH_Y_Color = 'MoNuSegTrainingData/Color_masks/'\n",
        "#TEST_PATH = 'Datasets/test/'\n",
        "Img_height = 1024\n",
        "Img_width  = 1024\n",
        "Img_channel = 3\n",
        "# Get train and test IDs\n",
        "train_ids_X = next(os.walk(TRAIN_PATH_X))[2]\n",
        "train_ids_y_binary = next(os.walk(TRAIN_PATH_Y_Binary))[2]\n",
        "train_ids_y_color = next(os.walk(TRAIN_PATH_Y_Color))[2]\n",
        "\n",
        "Img_Train_X = np.zeros((len(train_ids_X), Img_height, Img_width, Img_channel), dtype=np.uint8)\n",
        "Img_Train_Y_Binary = np.zeros((len(train_ids_y_binary), Img_height, Img_width), dtype=np.bool) ## size?\n",
        "Img_Train_Y_Color = np.zeros((len(train_ids_y_color), Img_height, Img_width,Img_channel), dtype=np.uint8)\n",
        "\n",
        "print('Resizing training images and masks')\n",
        "for n, id in tqdm(enumerate(train_ids_X), total=len(train_ids_X)):\n",
        "    path = TRAIN_PATH_X\n",
        "    path1 =  TRAIN_PATH_Y_Binary\n",
        "    path2 = TRAIN_PATH_Y_Color\n",
        "    img = imread(path + id)[:, :, :Img_channel]\n",
        "    img = resize(img, (Img_height, Img_width),\n",
        "                 mode='constant', preserve_range=True)\n",
        "    Img_Train_X[n] = img  # Fill empty X_train with values from img\n",
        "for n, id in tqdm(enumerate(train_ids_y_binary), total=len(train_ids_y_binary)):\n",
        "    mask = imread(path1 + id)[:, :]\n",
        "    mask = resize(mask, (Img_height, Img_width),\n",
        "                 mode='constant', preserve_range=True)\n",
        "    Img_Train_Y_Binary[n] = mask\n",
        "for n, id in tqdm(enumerate(train_ids_y_color), total=len(train_ids_y_color)):\n",
        "    mask = imread(path2 + id)[:, :,:Img_channel]\n",
        "    mask = resize(mask, (Img_height, Img_width),\n",
        "                 mode='constant', preserve_range=True)\n",
        "    Img_Train_Y_Color[n] = mask\n",
        "\n",
        "#test_ids = next(os.walk(TEST_PATH))[1]\n",
        "#4/1AY0e-g4YOvl3Q_7zUuXXPXLlUBzchPfETEeUI_0CpsNjPzfBHP5r_M07Jf0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fphuluZPmdee"
      },
      "source": [
        "image_x = random.randint(0, len(train_ids_X))\n",
        "imshow(Img_Train_X[image_x])\n",
        "print(Img_Train_X[image_x].shape)\n",
        "plt.show()\n",
        "imshow((Img_Train_Y_Binary[image_x]))\n",
        "print(Img_Train_Y_Binary[image_x].shape)\n",
        "plt.show()\n",
        "imshow((Img_Train_Y_Color[image_x]))\n",
        "plt.show()\n",
        "print(Img_Train_Y_Color[image_x].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oie-qD71ogea"
      },
      "source": [
        "Img_Train_XTrans = np.zeros(\n",
        "    (len(train_ids_X), Img_height, Img_width, Img_channel), dtype=np.uint8)\n",
        "Img_Train_XNorm = np.zeros(\n",
        "    (len(train_ids_X), Img_height, Img_width, Img_channel), dtype=np.uint8)\n",
        "transform1 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "for i in range(0,len(Img_Train_X)):\n",
        "  Img_Train_XTrans = transform(Img_Train_X[i])\n",
        "\n",
        "len( Img_Train_XTrans )\n",
        "mean, std = Img_Train_XTrans.mean([1,2]), Img_Train_XTrans.std([1,2])\n",
        "\n",
        "transform_norm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "  \n",
        "# get normalized image\n",
        "for i in range(0,len(Img_Train_X)):\n",
        "  img_normalized= transform_norm(Img_Train_X[i])\n",
        "  img_normalized = np.array(img_normalized)\n",
        "  Img_Train_XNorm[i] = img_normalized.transpose(1, 2, 0)\n",
        "\n",
        "len( Img_Train_XNorm)\n",
        "\n",
        "image_x = random.randint(0, len(train_ids_X))\n",
        "imshow(Img_Train_X[image_x])\n",
        "print(Img_Train_X[image_x].shape)\n",
        "plt.show()\n",
        "\n",
        "#imshow(Img_Train_XTrans[image_x])\n",
        "#print(Img_Train_XTrans[image_x].shape)\n",
        "#plt.show()\n",
        "\n",
        "imshow(Img_Train_XNorm[image_x])\n",
        "print(Img_Train_XNorm[image_x].shape)\n",
        "plt.show()\n",
        "\n",
        "Input = []\n",
        "for i in range(30):\n",
        "  x = torch.from_numpy(Img_Train_X[i]).float()\n",
        "  x = x.view(-1,3,1024,1024)\n",
        "  Input.append(x)\n",
        " # t = torch.from_numpy(Img_Train_Y_Binary[i]).float()\n",
        "  #t = t.view(-1,1,228,228)\n",
        "  #Target.append(t)\n",
        "\n",
        "Input = torch.cat(Input,dim = 0)\n",
        "#Target = torch.cat(Target,dim = 0)\n",
        "print(Input.shape)\n",
        "#print(Target.shape)\n",
        "\n",
        "#Training_set = Img_Train_X[:int(len(Img_Train_X)*0.8)]\n",
        "#Validation_set = Img_Train_X[int(len(Img_Train_X)*0.8):]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nj_LPey-H3V"
      },
      "source": [
        "# Object information: a set of bounding boxes [ymin, xmin, ymax, xmax] and their labels \n",
        "bbox0 = []\n",
        "for i in range(len(dataset)):\n",
        "  bbox0.append(dataset[i][1])\n",
        "labels = []\n",
        " # 0: background, 1: nuclei\n",
        "for i in range(len(dataset)):\n",
        "  labels.append(dataset[i][2])\n",
        "\n",
        "#Img = cv2.imread(Img_Train_X[image_x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iC0Kd8oHX28"
      },
      "source": [
        "bbox = np.concatenate(bbox0, axis=0).astype(int)\n",
        "bbox = np.asarray(bbox)\n",
        "print(bbox)\n",
        "#labels = np.concatenate(labels, axis=1).astype(int)\n",
        "labels = np.asarray(labels)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbI73Jrqtp0-"
      },
      "source": [
        "### Backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLl5Ck_5UXQV"
      },
      "source": [
        "class block(nn.Module):\n",
        "    def __init__(self, in_channels, intermediate_channels, shortcut = None, stride=1):\n",
        "        super(block, self).__init__()\n",
        "        self.expand = 4\n",
        "        self.conv1 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv2 = nn.Conv2d(intermediate_channels,intermediate_channels,kernel_size=3,stride=stride,padding=1,bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv3 = nn.Conv2d(intermediate_channels,intermediate_channels * self.expand,kernel_size=1,stride=1,padding=0,bias=True)\n",
        "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expand)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.shortcut = shortcut\n",
        "        self.stride = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.shortcut is not None:\n",
        "            shortcut = self.shortcut(shortcut)\n",
        "\n",
        "        x += shortcut\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet_50(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels):\n",
        "        super(ResNet_50, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.layer0 = nn.Sequential(nn.ZeroPad2d(3) , nn.Conv2d(image_channels, 64, kernel_size=7, stride=2,bias=True), nn.BatchNorm2d(64) , \n",
        "                                    nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "\n",
        "        self.layer1 = self.make_layer(block, layers[0], intermediate_channels=64, stride=1)\n",
        "        self.layer2 = self.make_layer(block, layers[1], intermediate_channels=128, stride=2)\n",
        "        self.layer3 = self.make_layer(block, layers[2], intermediate_channels=256, stride=2)\n",
        "        self.layer4 = self.make_layer(block, layers[3], intermediate_channels=512, stride=2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def stages(self):\n",
        "        return (self.layer0, self.layer1,self.layer2,self.layer3,self.layer4)\n",
        "\n",
        "    def make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
        "            identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels,intermediate_channels * 4,kernel_size=1,stride=stride,bias=True),nn.BatchNorm2d(intermediate_channels * 4),)\n",
        "\n",
        "        layers.append(block(self.in_channels, intermediate_channels, identity_downsample, stride))\n",
        "\n",
        "        self.in_channels = intermediate_channels * 4\n",
        "\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, intermediate_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "model = ResNet_50(block, layers = [3,4,6,3],image_channels=3)\n",
        "output = model.forward(Input)\n",
        "#output.shape\n",
        "# from torchsummary import summary\n",
        "#summary(ResNet_50(block, layers = [3,4,6,3],image_channels=3),(3,250,250))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRMeK4dhJNxz"
      },
      "source": [
        "# visualize the first 5 channels of the feature maps\n",
        "imgArray=output.data.cpu().numpy().squeeze(0)\n",
        "fig=plt.figure(figsize=(12, 4))\n",
        "figNo = 1\n",
        "for i in range(5): \n",
        "    fig.add_subplot(1, 5, figNo) \n",
        "    plt.imshow(imgArray[i], cmap='gray')\n",
        "    figNo +=1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GJV0AbV7Y1a"
      },
      "source": [
        "class Topdown(nn.Module):\n",
        "  def __init__(self, C1, C2, C3, C4, C5, out_channels):\n",
        "    super(Topdown, self).__init__()\n",
        "    self.out_channels = out_channels\n",
        "    self.C1 = C1\n",
        "    self.C2 = C2\n",
        "    self.C3 = C3\n",
        "    self.C4 = C4\n",
        "    self.C5 = C5\n",
        "\n",
        "\n",
        "    self.p5_conv1 = nn.Conv2d(2048, self.out_channels, kernel_size=1, stride=1,bias=True)  ### pool5\n",
        "    self.p4_conv1 = nn.Conv2d(1024, self.out_channels, kernel_size=1, stride=1,bias=True)  ## convolving c4\n",
        "    self.p3_conv1 = nn.Conv2d(512, self.out_channels, kernel_size=1, stride=1,bias=True)   ## convolving c3\n",
        "    self.p2_conv1 = nn.Conv2d(256, self.out_channels, kernel_size=1, stride=1,bias=True)   ## convolving c2\n",
        "    self.conv = nn.Conv2d(256, self.out_channels, kernel_size=3, stride=1, padding = 1,bias=True) ## convolution to find features\n",
        "\n",
        "  def forward(self,x):\n",
        "       x = self.C1(x)\n",
        "       x = self.C2(x)\n",
        "       C2Out = x\n",
        "       x = self.C3(x)\n",
        "       C3Out = x\n",
        "       x = self.C4(x)\n",
        "       C4Out = x\n",
        "       x = self.C5(x)\n",
        "       C5Out = x\n",
        "\n",
        "       P5 = self.p5_conv1(C5Out)\n",
        "       M4 = self.p4_conv1(C4Out) + func.upsample(P5, scale_factor = 2)\n",
        "       P4 = self.conv(M4)\n",
        "       M3 = self.p3_conv1(C3Out) + func.upsample(P4, scale_factor = 2)\n",
        "       P3 = self.conv(M3)\n",
        "       M2 = self.p2_conv1(C2Out) + func.upsample(P3, scale_factor = 2)\n",
        "       P2 = self.conv(M2)\n",
        "\n",
        "       return [P2,P3,P4,P5]\n",
        "\n",
        "#Resnet = ResNet_50(block,layers = [3,4,6,3],image_channels=3)\n",
        "#C1, C2, C3, C4, C5 = model.stages()\n",
        "\n",
        "#model1 = Topdown(C1, C2, C3, C4, C5, out_channels = 256)\n",
        "#P2,P3,P4,P5 = model1.forward(x)\n",
        "#print(P2.shape)\n",
        "#print(P3.shape)\n",
        "#print(P4.shape)\n",
        "#print(P5.shape)\n",
        "#from torchsummary import summary\n",
        "#summary(Topdown(C1, C2, C3, C4, C5, out_channels = 256),(3,250,250))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSZwWMxmJUwd"
      },
      "source": [
        "# visualize the first 5 channels of the feature maps\n",
        "imgArray=P5.data.cpu().numpy().squeeze(0)\n",
        "fig=plt.figure(figsize=(12, 4))\n",
        "figNo = 1\n",
        "for i in range(5): \n",
        "    fig.add_subplot(1, 5, figNo) \n",
        "    plt.imshow(imgArray[i], cmap='gray')\n",
        "    figNo +=1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rfHpYUqtxpR"
      },
      "source": [
        "###Generate Anchors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6ZYPENgM6j-"
      },
      "source": [
        "def GenerateAnchor(input_dims,sampling,ratios,scales):\n",
        "\n",
        "  pixel_size = input_dims//sampling\n",
        "  ctr_x = np.arange(sampling, (pixel_size+1) * sampling, sampling)\n",
        "  ctr_y = np.arange(sampling, (pixel_size+1) * sampling, sampling)\n",
        "  print(len(ctr_x), ctr_x)\n",
        "\n",
        "  index = 0\n",
        "  ctr = np.zeros(((pixel_size * pixel_size), 2))\n",
        "  for x in range(len(ctr_x)):\n",
        "      for y in range(len(ctr_y)):\n",
        "          ctr[index, 1] = ctr_x[x] - 8\n",
        "          ctr[index, 0] = ctr_y[y] - 8\n",
        "          index +=1\n",
        "  print(ctr.shape)\n",
        "\n",
        "\n",
        "  anchor_boxes = np.zeros( ((pixel_size * pixel_size * 9), 4))\n",
        "  index = 0\n",
        "  for c in ctr:\n",
        "      ctr_y, ctr_x = c\n",
        "      for i in range(len(ratios)):\n",
        "          for j in range(len(scales)):\n",
        "              h = sampling * scales[j] * np.sqrt(ratios[i])\n",
        "              w = sampling * scales[j] * np.sqrt(1./ ratios[i])\n",
        "              anchor_boxes[index, 0] = ctr_y - h / 2.\n",
        "              anchor_boxes[index, 1] = ctr_x - w / 2.\n",
        "              anchor_boxes[index, 2] = ctr_y + h / 2.\n",
        "              anchor_boxes[index, 3] = ctr_x + w / 2.\n",
        "              index += 1\n",
        "  print(anchor_boxes.shape)\n",
        "  return anchor_boxes\n",
        "\n",
        "#anchor_boxes = GenerateAnchor(1024,32,[0.5,1,2],[8,16,32])\n",
        "#print(a.shape)\n",
        "#print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRUVzxDFt-ng"
      },
      "source": [
        "def labels_locations(anchor_boxes,bbox,labels):\n",
        "  ground_box = bbox\n",
        "  ground_labels = labels\n",
        "\n",
        "  # Ignore cross-boundary anchor boxes\n",
        "  # valid anchor boxes with (y1, x1)>0 and (y2, x2)<=1024\n",
        "  index_inside = np.where((anchor_boxes[:, 0] >= 0) & (anchor_boxes[:, 1] >= 0) & (anchor_boxes[:, 2] <= 1024) &\n",
        "                (anchor_boxes[:, 3] <= 1024))[0]\n",
        "  print(index_inside.shape)\n",
        "\n",
        "  valid_anchor_boxes = anchor_boxes[index_inside]\n",
        "  print(valid_anchor_boxes.shape)\n",
        "\n",
        "\n",
        "\n",
        "  # Calculate iou of the valid anchor boxes  \n",
        "  ious = np.empty((len(valid_anchor_boxes), 2), dtype=np.float32)\n",
        "  ious.fill(0)\n",
        "  for num1, i in enumerate(valid_anchor_boxes):\n",
        "      ya1, xa1, ya2, xa2 = i  \n",
        "      anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
        "      for num2, j in enumerate(ground_box):\n",
        "          yb1, xb1, yb2, xb2 = j\n",
        "          box_area = (yb2- yb1) * (xb2 - xb1)\n",
        "          inter_x1 = max([xb1, xa1])\n",
        "          inter_y1 = max([yb1, ya1])\n",
        "          inter_x2 = min([xb2, xa2])\n",
        "          inter_y2 = min([yb2, ya2])\n",
        "          if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
        "              iter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
        "              iou = iter_area / (anchor_area+ box_area - iter_area)            \n",
        "          else:\n",
        "              iou = 0.\n",
        "          ious[num1, num2] = iou\n",
        "  print(ious.shape)\n",
        "\n",
        "\n",
        "  # What anchor box has max iou with the ground truth bbox  \n",
        "  gt_argmax_ious = ious.argmax(axis=0)\n",
        "  print(gt_argmax_ious)\n",
        "\n",
        "  gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
        "  print(gt_max_ious)\n",
        "\n",
        "  gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
        "  print(gt_argmax_ious)\n",
        "\n",
        "\n",
        "  # What ground truth bbox is associated with each anchor box \n",
        "  argmax_ious = ious.argmax(axis=1)\n",
        "  print(argmax_ious.shape)\n",
        "  print(argmax_ious)\n",
        "  max_ious = ious[np.arange(len(index_inside)), argmax_ious]\n",
        "  print(max_ious)\n",
        "\n",
        "  label = np.empty((len(index_inside), ), dtype=np.int32)\n",
        "  label.fill(-1)\n",
        "  print(label.shape)\n",
        "\n",
        "\n",
        "\n",
        "  # Use iou to assign 1 (objects) to two kind of anchors \n",
        "  # a) The anchors with the highest iou overlap with a ground-truth-box\n",
        "  # b) An anchor that has an IoU overlap higher than 0.7 with ground-truth box\n",
        "\n",
        "  # Assign 0 (background) to an anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes\n",
        "  pos_iou_threshold  = 0.7\n",
        "  neg_iou_threshold = 0.3\n",
        "  label[gt_argmax_ious] = 1\n",
        "  label[max_ious >= pos_iou_threshold] = 1\n",
        "  label[max_ious < neg_iou_threshold] = 0\n",
        "\n",
        "\n",
        "  n_sample = 256\n",
        "  pos_ratio = 0.5\n",
        "  n_pos = pos_ratio * n_sample\n",
        "  print(n_pos)\n",
        "  pos_index = np.where(label == 1)[0]\n",
        "  if len(pos_index) > n_pos:\n",
        "      disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
        "      label[disable_index] = -1\n",
        "      \n",
        "  n_neg = n_sample * np.sum(label == 1)\n",
        "  print(n_neg)\n",
        "  neg_index = np.where(label == 0)[0]\n",
        "  if len(neg_index) > n_neg:\n",
        "      disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)\n",
        "      label[disable_index] = -1\n",
        "\n",
        "\n",
        "\n",
        "  # For each valid anchor box, find the groundtruth object which has max_iou \n",
        "  max_iou_bbox = ground_box[argmax_ious]\n",
        "  print(max_iou_bbox.shape)\n",
        "\n",
        "  # valid anchor boxes h, w, cx, cy \n",
        "  height = valid_anchor_boxes[:, 2] - valid_anchor_boxes[:, 0]\n",
        "  width = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]\n",
        "  ctr_y = valid_anchor_boxes[:, 0] + 0.5 * height\n",
        "  ctr_x = valid_anchor_boxes[:, 1] + 0.5 * width\n",
        "\n",
        "  # valid anchor box max iou bbox h, w, cx, cy \n",
        "  base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
        "  base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
        "  base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\n",
        "  base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width\n",
        "\n",
        "  # valid anchor boxes çš„ loc = (y-ya/ha), (x-xa/wa), log(h/ha), log(w/wa)\n",
        "  eps = np.finfo(height.dtype).eps\n",
        "  height = np.maximum(height, eps)\n",
        "  width = np.maximum(width, eps)\n",
        "  dy = (base_ctr_y - ctr_y) / height\n",
        "  dx = (base_ctr_x - ctr_x) / width\n",
        "  dh = np.log(base_height / height)\n",
        "  dw = np.log(base_width / width)\n",
        "  anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
        "  print(anchor_locs.shape)\n",
        "\n",
        "\n",
        "  anchor_labels = np.empty((len(anchor_boxes),), dtype=label.dtype)\n",
        "  anchor_labels.fill(-1)\n",
        "  anchor_labels[index_inside] = label\n",
        "  anchor_labels = torch.from_numpy(anchor_labels)\n",
        "  print(anchor_labels.shape)\n",
        "\n",
        "  anchor_locations = np.empty((len(anchor_boxes),) + anchor_boxes.shape[1:], dtype=anchor_locs.dtype)\n",
        "  anchor_locations.fill(0)\n",
        "  anchor_locations[index_inside, :] = anchor_locs\n",
        "  anchor_locations = torch.from_numpy(anchor_locations)\n",
        "  print(anchor_locations.shape)\n",
        "\n",
        "  return anchor_labels,anchor_locations\n",
        "\n",
        "#anchor_labels,anchor_locs = labels_locations((GenerateAnchor(1024,32,[0.5,1,2],[8,16,32])),bbox0)\n",
        "#print(a.shape)\n",
        "#print(b.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAw3SSQIFaB1"
      },
      "source": [
        "### RPN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J62Z4sp2X4Vq"
      },
      "source": [
        "class RPN(nn.Module):\n",
        "  def __init__(self, anchor_no, inchannels,outchannels):\n",
        "    super(RPN,self).__init__()\n",
        "\n",
        "    self.anchor_no = anchor_no\n",
        "    self.conv = nn.Conv2d(inchannels,outchannels,kernel_size=3,stride=1,padding = 1, bias= True)\n",
        "   #self.conv.weight.data.normal_(0, 0.01)\n",
        "    self.conv_class = nn.Conv2d(outchannels,self.anchor_no*2,kernel_size = 1, stride = 1,  bias = True)\n",
        "    #self.conv_class.weight.data.normal_(0, 0.01)\n",
        "    self.conv_box = nn.Conv2d(outchannels,self.anchor_no*4,kernel_size = 1, stride = 1,  bias = True)\n",
        "   # self.conv_box.weight.data.normal_(0, 0.01)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x = self.conv(x)\n",
        "\n",
        "    self.region_class = self.conv_class(x)\n",
        "    \n",
        "    self.region_class = self.region_class.permute(0,2,3,1)\n",
        "    self.region_class = self.region_class.contiguous()\n",
        "    self.region_class = self.region_class.view(x.size()[0], -1, 2)\n",
        "    self.region_scores_objectiveness =  self.region_class.view(1, 9216, 2)[:, :, 1].contiguous().view(1,-1)\n",
        "\n",
        "    self.region_class_prob = self.softmax(self.region_class)\n",
        "   \n",
        "\n",
        "  \n",
        "\n",
        "    self.region_box = self.conv_box(x)\n",
        "    \n",
        "    self.region_box = self.region_box.permute(0,2,3,1)\n",
        "    self.region_box = self.region_box.contiguous()\n",
        "    self.region_box = self.region_box.view(x.size()[0], -1, 4)\n",
        "    print(self.region_class.shape,self.region_box.shape,self.region_class_prob.shape,self.region_scores_objectiveness.shape)\n",
        "\n",
        "   \n",
        "    return [self.region_class,self.region_class_prob,self.region_box,self.region_scores_objectiveness]\n",
        "#model2 = RPN(9, 256, 512)\n",
        "#pred_class,pred_prob,pred_box,pred_score = model2.forward(P5)\n",
        "\n",
        "#print(pred_class)\n",
        "#print(pred_score)\n",
        "#print(pred_prob)\n",
        "#from torchsummary import summary\n",
        "#summary(RPN(9, 256, 512),(256,50,50))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl9d3G2sQ2Ns"
      },
      "source": [
        "### RPN Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0jNhkczEvd7"
      },
      "source": [
        "def rpn_loss(rpn_prob,rpn_box, anchor_locations, anchor_labels):\n",
        "  rpn_loc = rpn_box[0]\n",
        "  rpn_score = rpn_prob[0]\n",
        "\n",
        "  gt_rpn_loc = anchor_locations\n",
        "  gt_rpn_score = anchor_labels\n",
        "\n",
        "  print(rpn_loc.shape, rpn_score.shape, gt_rpn_loc.shape, gt_rpn_score.shape)\n",
        "\n",
        "  # For classification we use cross-entropy loss\n",
        "  rpn_cls_loss = func.cross_entropy(rpn_score, gt_rpn_score.long(), ignore_index = -1)\n",
        "  print(rpn_cls_loss)\n",
        "\n",
        "  # For Regression we use smooth L1 loss as defined\n",
        "  pos = gt_rpn_score > 0\n",
        "  mask = pos.unsqueeze(1).expand_as(rpn_loc)\n",
        "  print(mask.shape)\n",
        "\n",
        "  # bounding boxes which have positve labels\n",
        "  mask_loc_preds = rpn_loc[mask].view(-1, 4)\n",
        "  mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)\n",
        "  print(mask_loc_preds.shape, mask_loc_targets.shape)\n",
        "\n",
        "  rpn_loc_loss = func.smooth_l1_loss(mask_loc_preds,mask_loc_targets)\n",
        "  print(rpn_loc_loss.sum())\n",
        "\n",
        "  # Combining both the rpn_cls_loss and rpn_reg_loss\n",
        "  rpn_lambda = 10.\n",
        "  N_reg = (gt_rpn_score >0).float().sum()\n",
        "  rpn_loc_loss = rpn_loc_loss.sum() / N_reg\n",
        "  rpn_loss = rpn_cls_loss + (rpn_lambda * rpn_loc_loss)\n",
        "  print(rpn_loss)\n",
        "\n",
        "  return rpn_loss\n",
        "#Loss = rpn_loss(pred_prob,pred_box,anchor_locs,anchor_labels)\n",
        "#print(Loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwpyBiY7Fe_C"
      },
      "source": [
        "### Region Proposal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6cGroBdFdvY"
      },
      "source": [
        "def RegionProposal(anchor_boxes, anchor_location, rpn_prob, rpn_box):\n",
        "  threshold_nms = 0.7  # non-maximum supression (NMS) \n",
        "  train_pre_nms = 8000 # no. of train pre-NMS\n",
        "  train_post_nms = 2000 # after nms, training Fast R-CNN using 2000 RPN proposals\n",
        "  test_pre_nms = 3000\n",
        "  test_post_nms = 300 # During testing we evaluate 300 proposals,\n",
        "  min_size = 32\n",
        "\n",
        " \n",
        "  # format converted from [y1, x1, y2, x2] to [ctr_x, ctr_y, h, w]\n",
        "  anchor_height = anchor_boxes[:, 2] - anchor_boxes[:, 0]\n",
        "  anchor_width = anchor_boxes[:, 3] - anchor_boxes[:, 1]\n",
        "  anchor_ctr_y = anchor_boxes[:, 0] + 0.5 * anchor_height\n",
        "  anchor_ctr_x = anchor_boxes[:, 1] + 0.5 * anchor_width\n",
        "  print(anchor_ctr_x.shape)\n",
        "\n",
        "\n",
        "  # format = (dy, dx, dh, dw)\n",
        "  rpn_box_numpy = rpn_box.cpu().data.numpy()\n",
        "  rpn_prob_numpy = rpn_prob.cpu().data.numpy()\n",
        "  dy = rpn_box_numpy[:, 0::4] # dy\n",
        "  dx = rpn_box_numpy[:, 1::4] # dx\n",
        "  dh = rpn_box_numpy[:, 2::4] # dh\n",
        "  dw = rpn_box_numpy[:, 3::4] # dw\n",
        "  print(dy.shape)\n",
        "  dy = torch.from_numpy(dy)\n",
        "  dy = dy.contiguous().view(-1,1)\n",
        "  dy = dy.numpy()\n",
        "  print(dy.shape)\n",
        "  dx= torch.from_numpy(dx)\n",
        "  dx = dx.contiguous().view(-1,1)\n",
        "  dx = dx.numpy()\n",
        "\n",
        "  dh = torch.from_numpy(dh)\n",
        "  dh = dh.contiguous().view(-1,1)\n",
        "  dh = dh.numpy()\n",
        "\n",
        "  dw = torch.from_numpy(dw)\n",
        "  dw = dw.contiguous().view(-1,1)\n",
        "  dw = dw.numpy()\n",
        "\n",
        "  # ctr_y = dy predicted by RPN * anchor_h + anchor_cy\n",
        "  # ctr_x similar\n",
        "  # h = exp(dh predicted by RPN) * anchor_h\n",
        "  # w similar\n",
        "  ctr_y = dy * anchor_height[:, np.newaxis] + anchor_ctr_y[:, np.newaxis]\n",
        "  ctr_x = dx * anchor_width[:, np.newaxis] + anchor_ctr_x[:, np.newaxis]\n",
        "  h = np.exp(dh) * anchor_height[:, np.newaxis]\n",
        "  w = np.exp(dw) * anchor_width[:, np.newaxis]\n",
        "  print(w.shape)\n",
        "  anchor_location_numpy = anchor_location.numpy()\n",
        "  # ROI = [y1, x1, y2, x2] \n",
        "  roi = np.zeros(rpn_box_numpy.shape, dtype=anchor_location_numpy.dtype)\n",
        "  print(roi.shape)\n",
        "  roi = torch.from_numpy(roi)\n",
        "  roi = roi.contiguous().view(-1,4)\n",
        "  roi = roi.numpy()\n",
        "\n",
        "  roi[:, 0::4] = ctr_y - 0.5 * h\n",
        "  roi[:, 1::4] = ctr_x - 0.5 * w\n",
        "  roi[:, 2::4] = ctr_y + 0.5 * h\n",
        "  roi[:, 3::4] = ctr_x + 0.5 * w\n",
        "  print(roi.shape)\n",
        "\n",
        "  # clip the predicted boxes to the image\n",
        "  img_size = (1024, 1024) #Image size\n",
        "  roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
        "  roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
        "  print(roi.shape, np.max(roi), np.min(roi))\n",
        "\n",
        "\n",
        "  # Remove predicted boxes with either height or width < threshold.\n",
        "  hs = roi[:, 2] - roi[:, 0]\n",
        "  ws = roi[:, 3] - roi[:, 1]\n",
        "  print(hs.shape,ws.shape)\n",
        "  keep = np.where((hs >= min_size) & (ws >= min_size))[0] #min_size=16\n",
        "  print(keep.shape)\n",
        "  roi = roi[keep, :]\n",
        "  print(roi.shape)\n",
        "  score = rpn_prob_numpy[:,keep]\n",
        "  print(keep.shape, roi.shape, score.shape)\n",
        "  score = torch.from_numpy(score)\n",
        "  score = score.view(1,keep.shape[0], 2)[:, :, 1].contiguous().view(1,-1)\n",
        "  score = score.numpy()\n",
        "  # Sort all (proposal, score) pairs by score from highest to lowest\n",
        "  order = score.ravel().argsort()[::-1]\n",
        "  print(order.shape)\n",
        "\n",
        "  #Take top pre_nms_topN (e.g. 12000 while training and 300 while testing)\n",
        "  order = order[:train_pre_nms]\n",
        "  roi = roi[order, :]\n",
        "  print(order.shape, roi.shape, roi.shape)\n",
        "\n",
        "  # Take all the roi boxes [roi_array]\n",
        "  y1 = roi[:, 0]\n",
        "  x1 = roi[:, 1]\n",
        "  y2 = roi[:, 2]\n",
        "  x2 = roi[:, 3]\n",
        "\n",
        "  # Find the areas of all the boxes [roi_area]\n",
        "  areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "\n",
        "  #Take the indexes of order the probability score in descending order \n",
        "  order = order.argsort()[::-1]\n",
        "  keep = []\n",
        "  while (order.size > 0):\n",
        "      i = order[0] #take the 1st elt in order and append to keep \n",
        "      keep.append(i)\n",
        "      xx1 = np.maximum(x1[i], x1[order[1:]]) \n",
        "      yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "      xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "      yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "      w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "      h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "      inter = w * h\n",
        "      ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "      inds = np.where(ovr <= threshold_nms)[0]\n",
        "      order = order[inds + 1]\n",
        "  keep = keep[:train_post_nms] # while training/testing , use accordingly\n",
        "  roi = roi[keep] # the final region proposals\n",
        "  print(len(keep), roi.shape)\n",
        "\n",
        "  return roi\n",
        "\n",
        "#roi = RegionProposal(anchor_boxes, anchor_locs, pred_prob, pred_box)\n",
        "\n",
        "## What will the input to the pred_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSnnmKzvFjua"
      },
      "source": [
        "### Region Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pu0bjLNICJM"
      },
      "source": [
        "def RegionTargets(bbox,labels,roi,Img):\n",
        "  n_sample = 128  # Number of samples from roi \n",
        "  pos_ratio = 0.25 # Number of positive examples out of the n_samples\n",
        "  pos_iou_thresh = 0.5  # Min iou of region proposal with any groundtruth object to consider it as positive label\n",
        "  neg_iou_thresh_high = 0.5  # iou 0~0.5 is considered as negative (0, background)\n",
        "  neg_iou_thresh_low = 0.0\n",
        "\n",
        "  # Find the iou of each ground truth object with the region proposals, \n",
        "  ious = np.empty((len(roi), 2), dtype=np.float32)\n",
        "  ious.fill(0)\n",
        "  for num1, i in enumerate(roi):\n",
        "      ya1, xa1, ya2, xa2 = i  \n",
        "      anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
        "      for num2, j in enumerate(bbox):\n",
        "          yb1, xb1, yb2, xb2 = j\n",
        "          box_area = (yb2- yb1) * (xb2 - xb1)\n",
        "          inter_x1 = max([xb1, xa1])\n",
        "          inter_y1 = max([yb1, ya1])\n",
        "          inter_x2 = min([xb2, xa2])\n",
        "          inter_y2 = min([yb2, ya2])\n",
        "          if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
        "              iter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
        "              iou = iter_area / (anchor_area+ box_area - iter_area)            \n",
        "          else:\n",
        "              iou = 0.\n",
        "          ious[num1, num2] = iou\n",
        "  print(ious.shape)\n",
        "\n",
        "  # Find out which ground truth has high IoU for each region proposal, Also find the maximum IoU\n",
        "  gt_assignment = ious.argmax(axis=1)\n",
        "  max_iou = ious.max(axis=1)\n",
        "  print(gt_assignment)\n",
        "  print(max_iou)\n",
        "\n",
        "  # Assign the labels to each proposal\n",
        "  gt_roi_label = labels[gt_assignment]\n",
        "  print(gt_roi_label)\n",
        "\n",
        "  # Select the foreground rois as per the pos_iou_thesh and \n",
        "  pos_roi_per_image = n_sample*pos_ratio \n",
        "  pos_index = np.where(max_iou >= pos_iou_thresh)[0]\n",
        "  pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
        "  if pos_index.size > 0:\n",
        "      pos_index = np.random.choice(pos_index, size=pos_roi_per_this_image, replace=False)\n",
        "  print(pos_roi_per_this_image)\n",
        "  print(pos_index)\n",
        "\n",
        "  # Similarly for negitive (background) region proposals\n",
        "  neg_index = np.where((max_iou < neg_iou_thresh_high) &\n",
        "                              (max_iou >= neg_iou_thresh_low))[0]\n",
        "  neg_roi_per_this_image = n_sample - pos_roi_per_this_image\n",
        "  neg_roi_per_this_image = int(min(neg_roi_per_this_image, neg_index.size))\n",
        "  if  neg_index.size > 0 :\n",
        "      neg_index = np.random.choice(neg_index, size=neg_roi_per_this_image, replace=False)\n",
        "  print(neg_roi_per_this_image)\n",
        "  print(neg_index)\n",
        "\n",
        "  # display ROI samples with postive \n",
        "  img_clone = np.copy(Img)\n",
        "  for i in range(pos_roi_per_this_image):\n",
        "      y0, x0, y1, x1 = roi[pos_index[i]].astype(int)\n",
        "      cv2.rectangle(img_clone, (x0, y0), (x1, y1), color=(255, 255, 2550), thickness=3) \n",
        "\n",
        "  for i in range(len(bbox)):\n",
        "      cv2.rectangle(img_clone, (bbox[i][1], bbox[i][0]), (bbox[i][3], bbox[i][2]), color=(0, 255, 0), thickness=3) # Draw Rectangle\n",
        "      \n",
        "  plt.imshow(img_clone)\n",
        "  plt.show()  \n",
        "\n",
        "  keep_index = np.append(pos_index, neg_index)\n",
        "  gt_roi_labels = gt_roi_label[keep_index]\n",
        "  gt_roi_labels[pos_roi_per_this_image:] = 0  # negative labels --> 0\n",
        "  sample_roi = roi[keep_index]\n",
        "  print(sample_roi.shape)\n",
        "\n",
        "  # Pick the ground truth objects for these sample_roi and \n",
        "  # later parameterize as we have done while assigning locations to anchor boxes \n",
        "  bbox_for_sampled_roi = bbox[gt_assignment[keep_index]]\n",
        "  print(bbox_for_sampled_roi.shape)\n",
        "\n",
        "  height = sample_roi[:, 2] - sample_roi[:, 0]\n",
        "  width = sample_roi[:, 3] - sample_roi[:, 1]\n",
        "  ctr_y = sample_roi[:, 0] + 0.5 * height\n",
        "  ctr_x = sample_roi[:, 1] + 0.5 * width\n",
        "\n",
        "  base_height = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]\n",
        "  base_width = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]\n",
        "  base_ctr_y = bbox_for_sampled_roi[:, 0] + 0.5 * base_height\n",
        "  base_ctr_x = bbox_for_sampled_roi[:, 1] + 0.5 * base_width\n",
        "\n",
        "  eps = np.finfo(height.dtype).eps\n",
        "  height = np.maximum(height, eps)\n",
        "  width = np.maximum(width, eps)\n",
        "\n",
        "  dy = (base_ctr_y - ctr_y) / height\n",
        "  dx = (base_ctr_x - ctr_x) / width\n",
        "  dh = np.log(base_height / height)\n",
        "  dw = np.log(base_width / width)\n",
        "\n",
        "  gt_roi_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
        "  print(gt_roi_locs.shape)\n",
        "\n",
        "  return sample_roi,gt_roi_locs\n",
        "\n",
        "#roi_labels,roi_locs = RegionTargets(bbox0,labels,roi,Img_Train_X[image_x])\n",
        "#roi_labels = torch.from_numpy(roi_labels)\n",
        "#roi_locs = torch.from_numpy(roi_locs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2iCwHJFOM1d"
      },
      "source": [
        "### Roi Align"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfP-5Mw0QOz2"
      },
      "source": [
        "def ROI_ALIGN(out_dims,inputs,roi_labels):\n",
        "  roialign = []\n",
        "  for i in range(len(inputs)):\n",
        "    feature_maps = inputs[i]\n",
        "    roi_align_obj = RoIAlign(out_dims, 2**-5, sampling_ratio= 4, aligned=False)\n",
        "    roi_align = roi_align_obj(feature_maps.type(torch.float32), [roi_labels.type(torch.float32)])\n",
        "    #roi_align = roi_align.detach().numpy()\n",
        "    roialign.append(roi_align)\n",
        "    #roi_align = torch.from_numpy(roi_align)\n",
        "    # Pack pooled features into one tensor\n",
        "  roialign = torch.cat(roialign, dim=0)\n",
        "\n",
        "  print(roialign.shape)\n",
        "  return roialign\n",
        "#eature_maps = [P2,P3,P4,P5]\n",
        "#roi_align1 = ROI_ALIGN(7,feature_maps,roi_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ93Dd0_DG2P"
      },
      "source": [
        "### ROI Allign"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk0Rso4zrk9Q"
      },
      "source": [
        "def bilinear_interpolate(img, x, y):\n",
        "    x0 = torch.floor(x).type(torch.int64)\n",
        "    x1 = x0 + 1\n",
        "\n",
        "    y0 = torch.floor(y).type(torch.int64)\n",
        "    y1 = y0 + 1\n",
        "\n",
        "    x0 = torch.clamp(x0, 0, img.shape[1]-1)\n",
        "    x1 = torch.clamp(x1, 0, img.shape[1]-1)\n",
        "    y0 = torch.clamp(y0, 0, img.shape[0]-1)\n",
        "    y1 = torch.clamp(y1, 0, img.shape[0]-1)\n",
        "\n",
        "    Ia = img[y0, x0]\n",
        "    Ib = img[y1, x0]\n",
        "    Ic = img[y0, x1]\n",
        "    Id = img[y1, x1]\n",
        "\n",
        "    norm_const = 1/((x1.type(torch.float32) - x0.type(torch.float32))*(y1.type(torch.float32) - y0.type(torch.float32)))\n",
        "\n",
        "    wa = (x1.type(torch.float32) - x) * (y1.type(torch.float32) - y) * norm_const\n",
        "    wb = (x1.type(torch.float32) - x) * (y-y0.type(torch.float32)) * norm_const\n",
        "    wc = (x-x0.type(torch.float32)) * (y1.type(torch.float32) - y) * norm_const\n",
        "    wd = (x-x0.type(torch.float32)) * (y - y0.type(torch.float32)) * norm_const\n",
        "\n",
        "    return torch.t(torch.t(Ia)*wa) + torch.t(torch.t(Ib)*wb) + torch.t(torch.t(Ic)*wc) + torch.t(torch.t(Id)*wd)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiARHJdYYwmQ"
      },
      "source": [
        "class TorchROIAlign(nn.Module):\n",
        "\n",
        "    def __init__(self, output_size, scaling_factor):\n",
        "        self.output_size = output_size\n",
        "        self.scaling_factor = scaling_factor\n",
        "\n",
        "    def _roi_align(self, features, scaled_proposal):\n",
        "        _, num_channels, h, w = features.shape\n",
        "\n",
        "        xp0, yp0, xp1, yp1 = scaled_proposal\n",
        "        p_width = xp1 - xp0\n",
        "        p_height = yp1 - yp0\n",
        "\n",
        "        w_stride = p_width/self.output_size\n",
        "        h_stride = p_height/self.output_size\n",
        "\n",
        "        interp_features = torch.zeros((num_channels, self.output_size, self.output_size))\n",
        "\n",
        "        for i in range(self.output_size):\n",
        "            for j in range(self.output_size):\n",
        "                x_strt = i*w_stride + xp0\n",
        "                y_strt = j*h_stride + yp0\n",
        "\n",
        "                # generate 4 points for interpolation\n",
        "                # notice no rounding\n",
        "                x1 = torch.Tensor([x_strt + 0.25*w_stride])\n",
        "                x2 = torch.Tensor([x_strt + 0.75*w_stride])\n",
        "                y1 = torch.Tensor([y_strt + 0.25*h_stride])\n",
        "                y2 = torch.Tensor([y_strt + 0.75*h_stride])\n",
        "\n",
        "                for c in range(num_channels):\n",
        "                    img = features[0,c]\n",
        "                    v1 = bilinear_interpolate(img, x1, y1)\n",
        "                    v2 = bilinear_interpolate(img, x1, y2)\n",
        "                    v3 = bilinear_interpolate(img, x2, y1)\n",
        "                    v4 = bilinear_interpolate(img, x2, y2)\n",
        "\n",
        "                    interp_features[c, j, i] = (v1+v2+v3+v4)/4\n",
        "        \n",
        "        return interp_features\n",
        "\n",
        "    def __call__(self, feature_layer, proposals):\n",
        "\n",
        "        _, num_channels, _, _ = feature_layer.shape\n",
        "        # first scale proposals down by self.scaling factor\n",
        "        scaled_proposals = torch.zeros_like(proposals)\n",
        "\n",
        "        # notice no ceil or floor functions\n",
        "        scaled_proposals[:, 0] = proposals[:, 0] * self.scaling_factor\n",
        "        scaled_proposals[:, 1] = proposals[:, 1] * self.scaling_factor\n",
        "        scaled_proposals[:, 2] = proposals[:, 2] * self.scaling_factor\n",
        "        scaled_proposals[:, 3] = proposals[:, 3] * self.scaling_factor\n",
        "\n",
        "        res = torch.zeros((len(proposals), num_channels, self.output_size,\n",
        "                        self.output_size))\n",
        "        for idx in tqdm(range(len(scaled_proposals))):\n",
        "            proposal = scaled_proposals[idx]\n",
        "            res[idx] = self._roi_align(feature_layer, proposal)\n",
        "\n",
        "        return res\n",
        "\n",
        "my_roi_align_obj = TorchROIAlign(7, 2**(-5))\n",
        "feat_layer = torch.randn(1, 64, 32, 32)\n",
        "roi_align1 = my_roi_align_obj(P5,roi_labels)\n",
        "print(roi_align1)\n",
        "print(roi_align1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SWem0B7QqZv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvh976sDDMW0"
      },
      "source": [
        "###MCRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfSZ1jd1wxSw"
      },
      "source": [
        "class classify(nn.Module):\n",
        "  def __init__(self, inchannels, num_classes):\n",
        "    super(classify,self).__init__()\n",
        "\n",
        "\n",
        "    self.conv1 = nn.Conv2d(inchannels,1024,kernel_size= 3,stride=1,bias= True)\n",
        "    self.bn = nn.BatchNorm2d(1024)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv2 = nn.Conv2d(1024,1024,kernel_size= 1,stride = 1, bias = True)\n",
        "\n",
        "    self.classify_box = nn.Linear(1024,num_classes*4)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.classify_classes = nn.Linear(1024, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "     x = self.conv1(x)\n",
        "     x = self.bn(x)\n",
        "     x = self.relu(x)\n",
        "     x = self.conv2(x)\n",
        "     x = self.bn(x)\n",
        "     x = self.relu(x)\n",
        "\n",
        "     x = x.view(-1,1024)\n",
        "     mrcnn_class = self.classify_classes(x)\n",
        "     mrcnn_class_prob = self.softmax(mrcnn_class)\n",
        "\n",
        "     mrcnn_box = self.classify_box(x)\n",
        "     mrcnn_box = mrcnn_box.view(mrcnn_box.size()[0], -1, 4)\n",
        "\n",
        "     return mrcnn_class, mrcnn_class_prob, mrcnn_box\n",
        "\n",
        "#roi_align_obj_1 = RoIAlign(7, 2**-5, sampling_ratio= 4, aligned=False)\n",
        "#roi_align1 = roi_align_obj_1(P5.type(torch.float32), [roi_labels.type(torch.float32)])\n",
        "#print(roi_align1.shape)\n",
        "#model7 = classify(256,2)\n",
        "\n",
        "#mrcnn_class, mrcnn_class_prob, mrcnn_box =  model7.forward(roi_align1)\n",
        "\n",
        "#print(mrcnn_class.shape)\n",
        "#print( mrcnn_class_prob.shape)\n",
        "#print( mrcnn_box)\n",
        "#summary(classify(256,10),(256,7,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpV1dJSC3Um0"
      },
      "source": [
        "# display bounding box and labels\n",
        "img0_clone = np.copy(Img_Train_X[image_x])\n",
        "mrcnn_box = mrcnn_box.view(mrcnn_box.size(0),-1)\n",
        "for i in range(len(mrcnn_box)):\n",
        "    cv2.rectangle(img0_clone, (mrcnn_box[i][1], mrcnn_box[i][0]), (mrcnn_box[i][3], mrcnn_box[i][2]), color=(255, 255, 2550), thickness=5) \n",
        "    #cv2.putText(img0_clone, str(int(labels[i])), (bbox0[i][3],bbox0[i][2]), cv2.FONT_HERSHEY_SIMPLEX, 3, (0,0,255),thickness=3) \n",
        "plt.imshow(img0_clone)\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8iW3wsDB4wk"
      },
      "source": [
        "class Mask_segmentation(nn.Module):\n",
        "    def __init__(self, inchannels, num_classes):\n",
        "        super(Mask_segmentation, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inchannels, 256, kernel_size=3, stride=1,padding = 1, bias = True)\n",
        "        self.bn = nn.BatchNorm2d(256)\n",
        "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding = 1, stride=1,bias = True)\n",
        "        self.deconv = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2,bias = True)\n",
        "        self.conv3 = nn.Conv2d(256, num_classes, kernel_size=1, stride=1, bias = True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.deconv(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "#roi_align_obj = RoIAlign(14, 2**-5, sampling_ratio= 4, aligned=False)\n",
        "#roi_align2 = roi_align_obj(P5.type(torch.float32), [roi_labels.type(torch.float32)])\n",
        "#feature_maps = [P2,P3,P4,P5]\n",
        "#roi_align2 = ROI_ALIGN(14,feature_maps,roi_labels)\n",
        "#print(roi_align2.shape)\n",
        "#mcrnn_seg = Mask_segmentation(256,80)\n",
        "#mcrnn_mask  = mcrnn_seg.forward(roi_align2)\n",
        "#print(mcrnn_mask.shape)\n",
        "#from torchsummary import summary\n",
        "#summary(Mask_segmentation(256,80),(256,14,14))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjXx2Cslv0Bi"
      },
      "source": [
        "targets = []\n",
        "for i in range(len(dataset)):\n",
        "  targets.append(dataset[i][3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1NMIEGfIbE7"
      },
      "source": [
        "def compute_mrcnn_class_loss(roi_labels,mrcnn_class):\n",
        "\n",
        "    # Loss\n",
        "    loss = func.cross_entropy(mrcnn_class,roi_labels.long())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def compute_mrcnn_box_loss(roi_locs, roi_labels,mrcnn_box):\n",
        "\n",
        "    if roi_labels.size():\n",
        "        # Only positive ROIs contribute to the loss. And only\n",
        "        # the right class_id of each ROI. Get their indicies.\n",
        "        positive_roi_ix = torch.nonzero(roi_labels > 0)[:, 0]\n",
        "        positive_roi_class_ids = roi_labels[positive_roi_ix.data].long()\n",
        "        indices = torch.stack((positive_roi_ix,positive_roi_class_ids), dim=1)\n",
        "        roi_locs = roi_locs[indices[:,0].data,:]\n",
        "        mrcnn_box = mrcnn_box[indices[:,0].data,indices[:,1].data,:]\n",
        "\n",
        "        # Smooth L1 loss\n",
        "        loss = func.smooth_l1_loss(mrcnn_box, roi_locs)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def compute_mrcnn_mask_loss(target_masks, roi_labels, mrcnn_mask):\n",
        "  \n",
        "    positive_ix = torch.nonzero(roi_labels > 0)[:, 0]\n",
        "    positive_class_ids = roi_labels[positive_ix.data].long()\n",
        "    indices = torch.stack((positive_ix, positive_class_ids), dim=1)\n",
        "\n",
        "    y_true = target_masks[indices[:,0].data,:,:]\n",
        "    y_pred = mrcnn_mask[indices[:,0].data,indices[:,1].data,:,:]\n",
        "\n",
        "    loss = func.binary_cross_entropy(y_pred, y_true)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "#loss = compute_mrcnn_class_loss(roi_labels,mrcnn_class) + compute_mrcnn_box_loss(roi_locs, roi_labels,mrcnn_box) + compute_mrcnn_mask_loss(target, roi_labels, mrcnn_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm5-BbYL4Zgx"
      },
      "source": [
        "def MRCNN(image,bbox,labels):\n",
        "\n",
        "  def initialize_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.xavier_uniform(m.weight)\n",
        "      if m.bias is not None:\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        m.weight.data.normal_(0, 0.01)\n",
        "        m.bias.data.zero_()\n",
        "  # Object information: a set of bounding boxes [ymin, xmin, ymax, xmax] and their labels \n",
        "  Resnet = ResNet_50(block, layers = [3,4,23,3],image_channels=3)\n",
        "  #output = model.forward(x)\n",
        "  C1, C2, C3, C4, C5 = Resnet.stages()\n",
        "\n",
        "  fpn = Topdown(C1, C2, C3, C4, C5, out_channels = 256)\n",
        "  \n",
        "  #P2,P3,P4,P5 = fpn.forward(x)\n",
        "\n",
        "  anchor_boxes = GenerateAnchor(1024,32,[0.5,1,2],[8,16,32])\n",
        "\n",
        "  anchor_labels,anchor_locs = labels_locations(anchor_boxes,bbox,labels)\n",
        "\n",
        "  rpn = RPN(9, 256, 512)\n",
        "\n",
        "  [P2,P3,P4,P5] = fpn(image)\n",
        "\n",
        "  feature_maps = [P2,P3,P4,P5]\n",
        "\n",
        "        # Loop through pyramid layers\n",
        "  layer_outputs = []  # list of lists\n",
        "  for p in feature_maps:\n",
        "    layer_outputs.append(rpn(p))\n",
        "  outputs = list(zip(*layer_outputs))\n",
        "  outputs = [torch.cat(list(o), dim=1) for o in outputs]\n",
        "  pred_class,pred_prob,pred_box,pred_score = outputs\n",
        "\n",
        "  roi = RegionProposal(anchor_boxes, anchor_locs, pred_prob, pred_box)\n",
        "\n",
        "  roi_labels,roi_locs = RegionTargets(bbox,labels,roi,image)\n",
        "  roi_labels = torch.from_numpy(roi_labels)\n",
        "  roi_locs = torch.from_numpy(roi_locs) \n",
        "\n",
        "  roi_align1 = ROI_ALIGN(7,feature_maps,roi_labels)\n",
        "  mrcnn_classify = classify(256,2)\n",
        "  mrcnn_class, mrcnn_class_prob, mrcnn_box =   mrcnn_classify .forward(roi_align1)\n",
        "\n",
        "  roi_align2 = ROI_ALIGN(14,feature_maps,roi_labels)\n",
        "  mcrnn_seg = Mask_segmentation(256,2)\n",
        "  mcrnn_mask  = mcrnn_seg.forward(roi_align2)\n",
        "  \n",
        "\n",
        "  rpn_Loss = rpn_loss(pred_prob,pred_box,anchor_locs,anchor_labels)\n",
        "  mrcnn_loss = compute_mrcnn_class_loss(roi_labels,mrcnn_class) + compute_mrcnn_box_loss(roi_locs, roi_labels,mrcnn_box) + compute_mrcnn_mask_loss(targets, roi_labels, mrcnn_mask)\n",
        "\n",
        "  \n",
        "  return rpn_Loss ,mrcnn_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V523YZzeRDvG"
      },
      "source": [
        "MRCNN(Img_Train_X[0],bbox = np.array([[160, 147, 260, 234], [139, 312, 200, 348]]),labels = np.array([1, 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47n34tHucn36"
      },
      "source": [
        "params = []\n",
        "params.append(Resnet.parameters())\n",
        "params.append(fpn.parameters())\n",
        "params.append(rpn.parameters())\n",
        "params.append(mcrnn_seg.parameters())\n",
        "  ## How to get params\n",
        "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztDUTmcpQXwL"
      },
      "source": [
        "import utils\n",
        "torch.manual_seed(1)\n",
        "dataset = Monuseg_Dataset('/content/MonuSeg',get_transform(train = True))\n",
        "dataset_test = Monuseg_Dataset('/content/MonuSeg', get_transform(train=True))\n",
        "\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "dataset = torch.utils.data.Subset(dataset, indices[0:450])\n",
        "\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[450:480])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KALRgU_8PxJV"
      },
      "source": [
        "#Backpropagation\n",
        "for i in range(10):\n",
        "  loss = MRCNN(dataset[0:10])\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLCQ26S1lvz1"
      },
      "source": [
        "## Size of target image\n",
        "## How to train the model(Like Implementation?)\n",
        "## How to decide the bouding boxes(min and max coordinates)\n",
        "## How to obatin the image from masks - (PIL Library)\n",
        "## The model is not running the session crashes\n",
        "## Which classes?\n",
        "## Loss fucntions\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}